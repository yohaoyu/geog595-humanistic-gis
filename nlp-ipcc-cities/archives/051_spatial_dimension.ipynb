{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "051_spatial_dimension.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XHn0E8Bnuc_0"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Create data on to Google Drive\n",
        "from google.colab import drive\n",
        "# Mount your Drive to the Colab VM.\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "CkiuWHS8uxj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read text file\n",
        "print(\"reading text file...\")\n",
        "processedTxtPath = \"/gdrive/MyDrive/geog595/gay-seattle-processed.txt\"\n",
        "with open(processedTxtPath, \"r\", encoding=\"utf8\") as txt_file:\n",
        "    txt = txt_file.read()\n"
      ],
      "metadata": {
        "id": "PYSE9n5yulxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert text to lowercase\n",
        "txt = txt.lower()\n",
        "# Remove numbers\n",
        "txt = re.sub(r'\\d+', '', txt)\n",
        "\n",
        "# Remove punctuation\n",
        "txt = re.sub(r'[^\\w\\s]', '', txt)\n",
        "\n",
        "# delete the white spaces\n",
        "# https://www.journaldev.com/23763/python-remove-spaces-from-string#python-remove-whitespaces-from-string-using-regex\n",
        "txt = \" \".join(txt.split())\n",
        "txt.translate({ord(c): None for c in string.whitespace})\n",
        "\n",
        "txt = txt.replace(\"gays\", \"gay\").replace(\"lesbians\", \"lesbian\").replace(\"seattles\", \"seattle\").replace(\"citys\", \"city\")\n",
        "print(txt)\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = len(txt)\n",
        "my_doc = nlp(txt)\n",
        "\n",
        "# tokenizing the text\n",
        "print(\"tokenizing the text...\")\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token)\n",
        "print(\"list of tokens:\", token_list)\n",
        "\n",
        "# removing stopwords\n",
        "print(\"removing stop words...\")\n",
        "filtered_sent = []\n",
        "for word in my_doc:\n",
        "    if not word.is_stop:\n",
        "        filtered_sent.append(word)\n",
        "print(\"Filtered words: \", filtered_sent)\n",
        "\n",
        "# lemmatization\n",
        "print(\"lemmatizing...\")\n",
        "lemma_sent = []\n",
        "for word in filtered_sent:\n",
        "    lemma_sent.append(word.lemma_)\n",
        "print(\"lemmatized data: \", lemma_sent)\n",
        "processedTxt = \" \".join(lemma_sent)\n",
        "processedDoc = nlp(processedTxt)\n",
        "\n",
        "geoTxt = \"\"\n",
        "for ent in processedDoc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "    if ent.label_ == \"GPE\":\n",
        "        geoTxt += ent.text.replace(\" \", \"\") + \" \"\n",
        "\n",
        "\n",
        "# tokenize and calculate the word frequencies\n",
        "nltk.download('punkt')\n",
        "tokens = nltk.tokenize.word_tokenize(geoTxt)\n",
        "fDist = FreqDist(tokens)\n",
        "with open(\"/gdrive/MyDrive/geog595/gay-seattle-places.csv\", \"w\", encoding=\"utf8\") as fp:\n",
        "    for item in fDist.most_common(300):\n",
        "        try:\n",
        "            fp.write(\"%s, %d\\n\" % (item[0].replace(\"county\", \" county\").replace(\"state\", \" state\").replace(\"city\", \" city\"), item[1]))\n",
        "            print(item)\n",
        "        except TypeError as error:\n",
        "            pass\n",
        "print(\"finished!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ENTJL7BOulm7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}