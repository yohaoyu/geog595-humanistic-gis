{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "030_model_buildler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hPnBDRuvrR9n"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk.data\n",
        "import pickle\n",
        "from multiprocessing import cpu_count\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Create data on to Google Drive\n",
        "from google.colab import drive\n",
        "# Mount your Drive to the Colab VM.\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FCZzH1lrcK1",
        "outputId": "e40062fe-7450-47e0-a283-9773a09ffa4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processedTxtPath = \"/gdrive/MyDrive/geog595/gay-seattle-processed.txt\"\n",
        "pickledTxtPath = \"/gdrive/MyDrive/geog595/gay-seattle-pickled.bin\"\n",
        "modelPath = \"/gdrive/MyDrive/geog595/gay-seattle.w2v\""
      ],
      "metadata": {
        "id": "KyO_xnj4ri9C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def review_to_wordlist(review, remove_stopwords=False):\n",
        "    # Function to convert a document to a sequence of words,\n",
        "    # optionally removing stop words.  Returns a list of words.\n",
        "    #\n",
        "    # 1. Remove HTML\n",
        "    # review_text = BeautifulSoup(review, 'html5lib').get_text()\n",
        "    #\n",
        "    # 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
        "    #\n",
        "    # 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "    #\n",
        "    # 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    #\n",
        "    # 5. Return a list of words\n",
        "    return words\n",
        "\n",
        "\n",
        "# Define a function to split a review into parsed sentences\n",
        "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
        "    # Function to split a review into parsed sentences. Returns a\n",
        "    # list of sentences, where each sentence is a list of words\n",
        "    #\n",
        "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    #\n",
        "    # 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "            # Otherwise, call review_to_wordlist to get a list of words\n",
        "            new_sentence = review_to_wordlist(raw_sentence, remove_stopwords)\n",
        "            if new_sentence != [] and new_sentence != [u'none']:\n",
        "                sentences.append(new_sentence)\n",
        "    # Return the list of sentences (each sentence is a list of words,\n",
        "    # so this returns a list of lists\n",
        "    return sentences\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tt4hY-9vrsDd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # download the punkt tokenizer for sentence splitting\n",
        "    # nltk.download()\n",
        "    # load the tokenizer\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt')\n",
        "    print('loading the tokenizer...')\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "    # read text file\n",
        "    print(\"reading text file...\")\n",
        "    with open(processedTxtPath, \"r\", encoding=\"utf8\") as txt_file:\n",
        "        text = txt_file.read()\n",
        "\n",
        "    pickedTxt = review_to_sentences(text, tokenizer, remove_stopwords=True)\n",
        "    data = [d for d in pickedTxt]\n",
        "    # convert python objects into string representation for later use\n",
        "    print(\"pickling the list...\")\n",
        "    with open(pickledTxtPath, \"wb+\") as fp:\n",
        "        pickle.dump(data, fp)\n",
        "\n",
        "    with open(pickledTxtPath, \"rb\") as fp:\n",
        "        doc = pickle.load(fp)\n",
        "\n",
        "    # train a model\n",
        "    print(\"creating a model...\")\n",
        "    model = Word2Vec(doc, workers=cpu_count())\n",
        "    model.save(modelPath)\n",
        "    print(\"completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPYaz4Mdr1Il",
        "outputId": "85ff2b52-3eb5-4e69-bb0d-1a3b07b4d0db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "loading the tokenizer...\n",
            "reading text file...\n",
            "pickling the list...\n",
            "creating a model...\n",
            "completed!\n"
          ]
        }
      ]
    }
  ]
}